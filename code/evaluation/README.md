# Quantum Circuit Evaluation

This folder contains tools and scripts for evaluating quantum circuits generated by language models. The evaluation system analyzes the quality, correctness, and performance of quantum circuits in solving optimization problems.

## Overview

The evaluation framework performs several key analyses:

- **Circuit Compilation**: Tests whether generated QASM code can be successfully parsed and compiled into valid quantum circuits
- **Correctness Validation**: Verifies that circuits produce expected quantum states and measurement outcomes
- **Performance Metrics**: Computes expectation values, probability distributions, and relative entropy measures
- **Statistical Analysis**: Generates comprehensive statistics and summaries across multiple model outputs

## Structure

```text
evaluation/
├── src/                          # Core evaluation modules
│   ├── evaluate.py              # Basic circuit parsing and execution
│   ├── evaluate_samples.py      # Batch evaluation with detailed metrics
│   ├── computations.py          # Quantum computation utilities
│   └── util.py                  # Helper functions
├── out/                         # Current evaluation outputs
├── out_original/               # Baseline evaluation results
├── evaluate.sh                 # Single circuit evaluation
├── evaluate_samples.sh         # Batch evaluation script
├── evaluate_slurm.sh          # SLURM cluster evaluation
├── evaluate_all_slurm.sh      # Multi-model SLURM evaluation
└── requirements.txt           # Python dependencies
```

## Key Features

### Circuit Validation

- Parses QASM3 code from JSON model outputs
- Validates circuit compilation using Qiskit
- Handles various output formats and error cases

### Quantum Metrics

- **Expectation Values**: Computes energy expectation values for optimization problems
- **State Fidelity**: Measures how close generated states are to optimal solutions  
- **Probability Distributions**: Analyzes measurement outcome probabilities
- **Relative Entropy**: Quantifies divergence from target distributions

### Statistical Reporting

- Success rates for circuit compilation
- Correctness metrics based on state overlap
- Performance distributions across samples
- Model comparison summaries

## Usage

### Single Circuit Evaluation

```bash
# Evaluate a single quantum circuit
./evaluate.sh
```

Edit the `path` variable in `evaluate.sh` to point to your JSON file containing the QASM circuit.

### Batch Evaluation

```bash
# Evaluate multiple circuits with detailed metrics
./evaluate_samples.sh
```

This processes all circuits in a JSON file and generates:

- Individual circuit metrics
- Summary statistics
- CSV reports for analysis

### Cluster Evaluation

For large-scale evaluation on SLURM clusters:

```bash
# Submit single evaluation job
sbatch evaluate_slurm.sh

# Evaluate all models in parallel
./evaluate_all_slurm.sh
```

## Input Format

The evaluation system expects JSON files with the following structure:

```json
{
  "model_output": "OPENQASM 3.0;\ninclude \"stdgates.inc\";\nqubit[4] q;\n..."
}
```

Where `model_output` contains the QASM3 quantum circuit code.

## Output Format

### Summary Statistics

```json
{
  "total_samples": 200,
  "compiled_successfully": 172,
  "correct_state_count": 11,
  "correct_samples": [36, 46, 61, ...],
  "compilation_rate": 0.86,
  "correctness_rate": 0.064,
  "mean_expectation_value": -2.45,
  "std_expectation_value": 1.23
}
```

### CSV Reports

Detailed per-sample metrics including:

- Sample ID
- Compilation success
- Expectation values
- Relative entropy
- Correctness flags

## Dependencies

Install required packages:

```bash
pip install -r requirements.txt
```

Key dependencies:

- **Qiskit**: Quantum circuit simulation and analysis
- **PennyLane**: Additional quantum computing framework
- **NumPy**: Numerical computations
- **qiskit-qasm3-import**: QASM3 parsing support

## Configuration

### Evaluation Parameters

Key parameters in `evaluate_samples.py`:

- `RANDOM_SAMPLING_AMOUNT = 1000`: Number of random samples for statistical analysis
- `EXPECTATION_VALUE_DIFFERENCE_THRESHOLD = 0.2`: Threshold for correctness validation

### SLURM Settings

Cluster job parameters in `evaluate_slurm.sh`:

- **Time limit**: 2 hours
- **Memory**: 50GB
- **CPUs**: 6 cores
- **Nodes**: 1

## Evaluation Metrics

### Compilation Rate

Percentage of generated circuits that successfully compile and execute.

### Correctness Rate

Percentage of circuits that produce states within the acceptable threshold of the optimal solution.

### Performance Metrics

- Mean and standard deviation of expectation values
- Distribution of relative entropy measures
- State overlap with target solutions

## Model Comparison

The system supports evaluation of multiple language models:

- CodeGemma-7B-IT
- DeepSeek-R1-Distill-Qwen-1.5B  
- Gemma-3-4B-IT
- Llama-3.2-3B-Instruct
- Qwen2.5-3B-Instruct
- Custom quantum-circuit-qubo models

Results are organized by model name and evaluation mode (standard vs few-shot).

## Troubleshooting

### Common Issues

1. **QASM Parsing Errors**: Check that model output contains valid QASM3 syntax
2. **Memory Issues**: Increase SLURM memory allocation for large circuits
3. **Import Errors**: Ensure all dependencies are installed with correct versions

### Debug Mode

Add debug flags to evaluation scripts:

```bash
python3 src/evaluate_samples.py $path $out_path $model --debug
```

## Contributing

When adding new evaluation metrics:

1. Add computation functions to `computations.py`
2. Update the main evaluation loop in `evaluate_samples.py`  
3. Extend output formatting in summary generation
4. Update this README with new metric descriptions