The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) triton/2024.1-gcc
Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.2)
Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.0.0)
Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.6)
Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.14.0)
Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.3.1)
Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.7.1)
Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (4.55.0)
Requirement already satisfied: trl in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.15.2)
Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.17.0)
Requirement already satisfied: wandb in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.21.0)
Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.9.0)
Requirement already satisfied: hf-transfer in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.1.9)
Requirement already satisfied: qiskit in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.1.1)
Requirement already satisfied: qiskit_aer in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.17.1)
Requirement already satisfied: qiskit_algorithms in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (0.3.1)
Requirement already satisfied: qiskit_qasm3_import in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (0.6.0)
Requirement already satisfied: vllm in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.10.0)
Requirement already satisfied: deepspeed in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (0.17.4)
Requirement already satisfied: ray in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (2.48.0)
Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.18.0)
Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (21.0.0)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)
Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.4)
Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)
Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)
Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2025.3.0)
Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.34.3)
Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.2)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (3.12.15)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->-r requirements.txt (line 3)) (1.3.0)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (4.14.1)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (79.0.1)
Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (3.5)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.77)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.77)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.80)
Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (9.5.1.17)
Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (11.3.0.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (10.3.7.77)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (11.7.1.2)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.5.4.2)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (0.6.3)
Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (2.26.2)
Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.77)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.85)
Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (1.11.1.6)
Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (3.3.1)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 6)) (2025.7.34)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 6)) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 6)) (0.6.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets->-r requirements.txt (line 1)) (1.1.7)
Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from trl->-r requirements.txt (line 7)) (14.1.0)
Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft->-r requirements.txt (line 8)) (7.0.0)
Requirement already satisfied: click!=8.0.0,>=7.1 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (8.2.1)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (3.1.45)
Requirement already satisfied: platformdirs in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (4.3.8)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (6.31.1)
Requirement already satisfied: pydantic<3 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (2.11.7)
Requirement already satisfied: sentry-sdk>=2.0.0 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (2.34.1)
Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (2.33.2)
Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (0.4.1)
Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2025.8.3)
Requirement already satisfied: rustworkx>=0.15.0 in ./.venv/lib/python3.12/site-packages (from qiskit->-r requirements.txt (line 12)) (0.16.0)
Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.12/site-packages (from qiskit->-r requirements.txt (line 12)) (1.16.1)
Requirement already satisfied: stevedore>=3.0.0 in ./.venv/lib/python3.12/site-packages (from qiskit->-r requirements.txt (line 12)) (5.4.1)
Requirement already satisfied: openqasm3<2.0,>=0.4 in ./.venv/lib/python3.12/site-packages (from openqasm3[parser]<2.0,>=0.4->qiskit_qasm3_import->-r requirements.txt (line 15)) (1.0.1)
Requirement already satisfied: antlr4_python3_runtime<4.14,>=4.7 in ./.venv/lib/python3.12/site-packages (from openqasm3[parser]<2.0,>=0.4->qiskit_qasm3_import->-r requirements.txt (line 15)) (4.13.2)
Requirement already satisfied: cachetools in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (6.1.0)
Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.2.0)
Requirement already satisfied: blake3 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.0.5)
Requirement already satisfied: py-cpuinfo in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (9.0.0)
Requirement already satisfied: fastapi>=0.115.0 in ./.venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.116.1)
Requirement already satisfied: openai<=1.90.0,>=1.87.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.90.0)
Requirement already satisfied: prometheus_client>=0.18.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.22.1)
Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (11.3.0)
Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (7.1.0)
Requirement already satisfied: tiktoken>=0.6.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.10.0)
Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.10.12)
Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.7.30)
Requirement already satisfied: outlines_core==0.2.10 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.2.10)
Requirement already satisfied: diskcache==5.6.3 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (5.6.3)
Requirement already satisfied: lark==1.2.2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.2.2)
Requirement already satisfied: xgrammar==0.1.21 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.1.21)
Requirement already satisfied: partial-json-parser in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.2.1.1.post6)
Requirement already satisfied: pyzmq>=25.0.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (27.0.1)
Requirement already satisfied: msgspec in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.19.0)
Requirement already satisfied: gguf>=0.13.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.17.1)
Requirement already satisfied: mistral_common>=1.8.2 in ./.venv/lib/python3.12/site-packages (from mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (1.8.3)
Requirement already satisfied: opencv-python-headless>=4.11.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (4.12.0.88)
Requirement already satisfied: six>=1.16.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.17.0)
Requirement already satisfied: einops in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.8.1)
Requirement already satisfied: compressed-tensors==0.10.2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.10.2)
Requirement already satisfied: depyf==0.19.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.19.0)
Requirement already satisfied: cloudpickle in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (3.1.1)
Requirement already satisfied: watchfiles in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.1.0)
Requirement already satisfied: python-json-logger in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (3.3.0)
Requirement already satisfied: ninja in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.11.1.4)
Requirement already satisfied: pybase64 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.4.2)
Requirement already satisfied: cbor2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (5.6.5)
Requirement already satisfied: numba==0.61.2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.61.2)
Requirement already satisfied: torchaudio==2.7.1 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (2.7.1)
Requirement already satisfied: torchvision==0.22.1 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.22.1)
Requirement already satisfied: xformers==0.0.31 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.0.31)
Requirement already satisfied: astor in ./.venv/lib/python3.12/site-packages (from depyf==0.19.0->vllm->-r requirements.txt (line 16)) (0.8.1)
Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba==0.61.2->vllm->-r requirements.txt (line 16)) (0.44.0)
Requirement already satisfied: interegular>=0.3.2 in ./.venv/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm->-r requirements.txt (line 16)) (0.3.3)
Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (4.10.0)
Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (1.9.0)
Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (0.28.1)
Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (0.10.0)
Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (1.3.1)
Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (1.0.9)
Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (0.16.0)
Requirement already satisfied: hjson in ./.venv/lib/python3.12/site-packages (from deepspeed->-r requirements.txt (line 17)) (3.1.0)
Requirement already satisfied: msgpack in ./.venv/lib/python3.12/site-packages (from deepspeed->-r requirements.txt (line 17)) (1.1.1)
Requirement already satisfied: jsonschema in ./.venv/lib/python3.12/site-packages (from ray->-r requirements.txt (line 18)) (4.25.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (6.6.3)
Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.20.1)
Requirement already satisfied: starlette<0.48.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.47.2)
Requirement already satisfied: fastapi-cli>=0.0.8 in ./.venv/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.0.8)
Requirement already satisfied: python-multipart>=0.0.18 in ./.venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.0.20)
Requirement already satisfied: email-validator>=2.0.0 in ./.venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (2.2.0)
Requirement already satisfied: uvicorn>=0.12.0 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.35.0)
Requirement already satisfied: dnspython>=2.0.0 in ./.venv/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (2.7.0)
Requirement already satisfied: typer>=0.15.1 in ./.venv/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.16.0)
Requirement already satisfied: rich-toolkit>=0.14.8 in ./.venv/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.14.9)
Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in ./.venv/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.1.5)
Requirement already satisfied: rignore>=0.5.1 in ./.venv/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.6.4)
Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (4.0.12)
Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (5.0.2)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 5)) (3.0.2)
Requirement already satisfied: pydantic-extra-types>=2.10.5 in ./.venv/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (2.10.5)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema->ray->-r requirements.txt (line 18)) (2025.4.1)
Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema->ray->-r requirements.txt (line 18)) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema->ray->-r requirements.txt (line 18)) (0.27.0)
Requirement already satisfied: pycountry>=23 in ./.venv/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (24.6.1)
Requirement already satisfied: cupy-cuda12x in ./.venv/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm->-r requirements.txt (line 16)) (13.5.1)
Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->trl->-r requirements.txt (line 7)) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->trl->-r requirements.txt (line 7)) (2.19.2)
Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 7)) (0.1.2)
Requirement already satisfied: pbr>=2.0.0 in ./.venv/lib/python3.12/site-packages (from stevedore>=3.0.0->qiskit->-r requirements.txt (line 12)) (6.1.1)
Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (1.5.4)
Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.6.4)
Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (1.1.1)
Requirement already satisfied: uvloop>=0.15.1 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.21.0)
Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (15.0.1)
Requirement already satisfied: fastrlock>=0.5 in ./.venv/lib/python3.12/site-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm->-r requirements.txt (line 16)) (0.8.3)
Requirement already satisfied: soundfile>=0.12.1 in ./.venv/lib/python3.12/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (0.13.1)
Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (1.17.1)
Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (2.22)
Requirement already satisfied: soxr>=0.5.0 in ./.venv/lib/python3.12/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (0.5.0.post1)
ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:13:15,681 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:13:15,955 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 4, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:13:15,980 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 7, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:13:15,985 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 3, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:13:16,005 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 1, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
2025-08-11 12:13:16,005 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 6, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
2025-08-11 12:13:16,005 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 2, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
2025-08-11 12:13:16,005 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_121302/runs/Aug11_12-13-14_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 5, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': True, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 149.36it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 155.07it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 158.30it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 157.77it/s]
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 159.60it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 155.41it/s]loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json

Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

All model checkpoint weights were used when initializing Qwen3ForCausalLM.

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 160.18it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|| 5/5 [00:00<00:00, 167.77it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[2025-08-11 12:13:19,957] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:13:20,111] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.


Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-08-11 12:13:21,084] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using auto half precision backend
2025-08-11 12:13:21,244 - INFO - Resume checkpoint: None (fresh run)
[2025-08-11 12:13:21,274] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:13:21,275] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:13:21,275] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:13:21,275] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:13:21,332] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:13:21,332] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:13:21,333] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
The following columns in the Training set don't have a corresponding argument in `Qwen3ForCausalLM.forward` and have been ignored: problem_specific_attributes, graph, signature, text, circuit_with_params, adaptive_process, problem_type, number_of_qubits, cost_hamiltonian, number_of_layers, ansatz_id, circuit_with_symbols, exact_solution, optimization_type, solution. If problem_specific_attributes, graph, signature, text, circuit_with_params, adaptive_process, problem_type, number_of_qubits, cost_hamiltonian, number_of_layers, ansatz_id, circuit_with_symbols, exact_solution, optimization_type, solution are not expected by `Qwen3ForCausalLM.forward`,  you can safely ignore this message.
/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1805: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1805: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1811: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
***** Running training *****
  Num examples = 13,914
  Num Epochs = 15
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 26,100
  Number of trainable parameters = 1,023,841,920
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: benyucong (benyucong-aalto-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /scratch/cs/adis/yuc10/quantum-code-generation/code/training/wandb/run-20250811_121655-27mltvyq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-planet-3
wandb:  View project at https://wandb.ai/benyucong-aalto-university/quantum-circuit-generation
wandb:  View run at https://wandb.ai/benyucong-aalto-university/quantum-circuit-generation/runs/27mltvyq
  0%|          | 0/26100 [00:00<?, ?it/s][rank3]: Traceback (most recent call last):
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank3]:     train()
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank3]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank3]:     return inner_training_loop(
[rank3]:            ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank3]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank3]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank3]:     self.accelerator.backward(loss, **kwargs)
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank3]:     loss.backward(**kwargs)
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank3]:     torch.autograd.backward(
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank3]:     _engine_run_backward(
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank3]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank3]:     frame.check_recomputed_tensors_match(gid)
[rank3]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank3]:     raise CheckpointError(
[rank3]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank3]: tensor at position 29:
[rank3]: saved metadata: {'shape': torch.Size([2930, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([5860, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: tensor at position 30:
[rank3]: saved metadata: {'shape': torch.Size([2930, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}
[rank3]: recomputed metadata: {'shape': torch.Size([5860, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=3)}

[rank5]: Traceback (most recent call last):
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank5]:     train()
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank5]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank5]:     return inner_training_loop(
[rank5]:            ^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank5]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank5]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank5]:     self.accelerator.backward(loss, **kwargs)
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank5]:     loss.backward(**kwargs)
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank5]:     torch.autograd.backward(
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank5]:     _engine_run_backward(
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank5]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank5]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank5]:     frame.check_recomputed_tensors_match(gid)
[rank5]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank5]:     raise CheckpointError(
[rank5]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank5]: tensor at position 29:
[rank5]: saved metadata: {'shape': torch.Size([2875, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=5)}
[rank5]: recomputed metadata: {'shape': torch.Size([5750, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=5)}
[rank5]: tensor at position 30:
[rank5]: saved metadata: {'shape': torch.Size([2875, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=5)}
[rank5]: recomputed metadata: {'shape': torch.Size([5750, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=5)}

[rank4]: Traceback (most recent call last):
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank4]:     train()
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank4]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank4]:     return inner_training_loop(
[rank4]:            ^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank4]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank4]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank4]:     self.accelerator.backward(loss, **kwargs)
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank4]:     loss.backward(**kwargs)
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank4]:     torch.autograd.backward(
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank4]:     _engine_run_backward(
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank4]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank4]:     frame.check_recomputed_tensors_match(gid)
[rank4]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank4]:     raise CheckpointError(
[rank4]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank4]: tensor at position 29:
[rank4]: saved metadata: {'shape': torch.Size([614, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=4)}
[rank4]: recomputed metadata: {'shape': torch.Size([1228, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=4)}
[rank4]: tensor at position 30:
[rank4]: saved metadata: {'shape': torch.Size([614, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=4)}
[rank4]: recomputed metadata: {'shape': torch.Size([1228, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=4)}

[rank1]: Traceback (most recent call last):
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank1]:     train()
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank1]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank1]:     return inner_training_loop(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank1]:     self.accelerator.backward(loss, **kwargs)
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank1]:     loss.backward(**kwargs)
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank1]:     torch.autograd.backward(
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank1]:     _engine_run_backward(
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank1]:     frame.check_recomputed_tensors_match(gid)
[rank1]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank1]:     raise CheckpointError(
[rank1]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank1]: tensor at position 29:
[rank1]: saved metadata: {'shape': torch.Size([645, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([1290, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: tensor at position 30:
[rank1]: saved metadata: {'shape': torch.Size([645, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}
[rank1]: recomputed metadata: {'shape': torch.Size([1290, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=1)}

[rank2]: Traceback (most recent call last):
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank2]:     train()
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank2]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank2]:     return inner_training_loop(
[rank2]:            ^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank2]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank2]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank2]:     self.accelerator.backward(loss, **kwargs)
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank2]:     frame.check_recomputed_tensors_match(gid)
[rank2]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank2]:     raise CheckpointError(
[rank2]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank2]: tensor at position 29:
[rank2]: saved metadata: {'shape': torch.Size([1816, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([3632, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: tensor at position 30:
[rank2]: saved metadata: {'shape': torch.Size([1816, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}
[rank2]: recomputed metadata: {'shape': torch.Size([3632, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=2)}

[rank6]: Traceback (most recent call last):
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank6]:     train()
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank6]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank6]:     return inner_training_loop(
[rank6]:            ^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank6]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank6]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank6]:     self.accelerator.backward(loss, **kwargs)
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank6]:     loss.backward(**kwargs)
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank6]:     torch.autograd.backward(
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank6]:     _engine_run_backward(
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank6]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank6]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank6]:     frame.check_recomputed_tensors_match(gid)
[rank6]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank6]:     raise CheckpointError(
[rank6]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank6]: tensor at position 29:
[rank6]: saved metadata: {'shape': torch.Size([1198, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
[rank6]: recomputed metadata: {'shape': torch.Size([2396, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
[rank6]: tensor at position 30:
[rank6]: saved metadata: {'shape': torch.Size([1198, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}
[rank6]: recomputed metadata: {'shape': torch.Size([2396, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=6)}

[rank7]: Traceback (most recent call last):
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank7]:     train()
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank7]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank7]:     return inner_training_loop(
[rank7]:            ^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank7]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank7]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank7]:     self.accelerator.backward(loss, **kwargs)
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank7]:     loss.backward(**kwargs)
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank7]:     torch.autograd.backward(
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank7]:     _engine_run_backward(
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank7]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank7]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank7]:     frame.check_recomputed_tensors_match(gid)
[rank7]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank7]:     raise CheckpointError(
[rank7]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank7]: tensor at position 29:
[rank7]: saved metadata: {'shape': torch.Size([4662, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=7)}
[rank7]: recomputed metadata: {'shape': torch.Size([9324, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=7)}
[rank7]: tensor at position 30:
[rank7]: saved metadata: {'shape': torch.Size([4662, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=7)}
[rank7]: recomputed metadata: {'shape': torch.Size([9324, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=7)}

Traceback (most recent call last):
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
    train()
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
    trainer.train(resume_from_checkpoint=resume_ckpt)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
    loss.backward(**kwargs)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
    torch.autograd.backward(
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
    _engine_run_backward(
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
    frame.check_recomputed_tensors_match(gid)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
    raise CheckpointError(
torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
tensor at position 29:
saved metadata: {'shape': torch.Size([916, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([1832, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
tensor at position 30:
saved metadata: {'shape': torch.Size([916, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
recomputed metadata: {'shape': torch.Size([1832, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}

[rank0]: Traceback (most recent call last):
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 135, in <module>
[rank0]:     train()
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/sft_8b.py", line 124, in train
[rank0]:     trainer.train(resume_from_checkpoint=resume_ckpt)
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2229, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 2582, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/transformers/trainer.py", line 3845, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py", line 2578, in backward
[rank0]:     loss.backward(**kwargs)
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/_tensor.py", line 648, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py", line 353, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/autograd/graph.py", line 824, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 1128, in unpack_hook
[rank0]:     frame.check_recomputed_tensors_match(gid)
[rank0]:   File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py", line 902, in check_recomputed_tensors_match
[rank0]:     raise CheckpointError(
[rank0]: torch.utils.checkpoint.CheckpointError: torch.utils.checkpoint: Recomputed values for the following tensors have different metadata than during the forward pass.
[rank0]: tensor at position 29:
[rank0]: saved metadata: {'shape': torch.Size([916, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([1832, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: tensor at position 30:
[rank0]: saved metadata: {'shape': torch.Size([916, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}
[rank0]: recomputed metadata: {'shape': torch.Size([1832, 8, 128]), 'dtype': torch.bfloat16, 'device': device(type='cuda', index=0)}

[1;34mwandb[0m: 
[1;34mwandb[0m:  View run [33mwild-planet-3[0m at: [34mhttps://wandb.ai/benyucong-aalto-university/quantum-circuit-generation/runs/27mltvyq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250811_121655-27mltvyq/logs[0m
W0811 12:17:07.684000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194781 closing signal SIGTERM
W0811 12:17:07.685000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194782 closing signal SIGTERM
W0811 12:17:07.686000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194783 closing signal SIGTERM
W0811 12:17:07.686000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194784 closing signal SIGTERM
W0811 12:17:07.686000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194785 closing signal SIGTERM
W0811 12:17:07.687000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194786 closing signal SIGTERM
W0811 12:17:07.687000 194713 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 194788 closing signal SIGTERM
E0811 12:17:08.552000 194713 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 6 (pid: 194787) of binary: /scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/bin/python
Traceback (most recent call last):
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1186, in launch_command
    multi_gpu_launcher(args)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/commands/launch.py", line 815, in multi_gpu_launcher
    distrib_run.run(args)
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/distributed/run.py", line 883, in run
    elastic_launch(
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 139, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/torch/distributed/launcher/api.py", line 270, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
sft_8b.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-11_12:17:07
  host      : gpu59.int.triton.aalto.fi
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 194787)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
