The following modules were not unloaded:
  (Use "module --force purge" to unload all):

  1) triton/2024.1-gcc
Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (25.2)
Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.0.0)
Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.6)
Requirement already satisfied: sympy in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (1.14.0)
Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.3.1)
Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (2.7.1)
Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (4.55.0)
Requirement already satisfied: trl in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.15.2)
Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.17.0)
Requirement already satisfied: wandb in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (0.21.0)
Requirement already satisfied: accelerate in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (1.9.0)
Requirement already satisfied: hf-transfer in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.1.9)
Requirement already satisfied: qiskit in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (2.1.1)
Requirement already satisfied: qiskit_aer in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (0.17.1)
Requirement already satisfied: qiskit_algorithms in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (0.3.1)
Requirement already satisfied: qiskit_qasm3_import in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (0.6.0)
Requirement already satisfied: vllm in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.10.0)
Requirement already satisfied: deepspeed in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (0.17.4)
Requirement already satisfied: ray in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (2.48.0)
Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.18.0)
Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (21.0.0)
Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)
Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.4)
Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)
Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)
Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)
Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2025.3.0)
Requirement already satisfied: huggingface-hub>=0.24.0 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.34.3)
Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (25.0)
Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.2)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (3.12.15)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy->-r requirements.txt (line 3)) (1.3.0)
Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)
Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)
Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (4.14.1)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (79.0.1)
Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (3.5)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (3.1.6)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.77)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.77)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.80)
Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (9.5.1.17)
Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.4.1)
Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (11.3.0.4)
Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (10.3.7.77)
Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (11.7.1.2)
Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.5.4.2)
Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (0.6.3)
Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (2.26.2)
Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.77)
Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (12.6.85)
Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (1.11.1.6)
Requirement already satisfied: triton==3.3.1 in ./.venv/lib/python3.12/site-packages (from torch->-r requirements.txt (line 5)) (3.3.1)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 6)) (2025.7.34)
Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 6)) (0.21.4)
Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 6)) (0.6.1)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets->-r requirements.txt (line 1)) (1.1.7)
Requirement already satisfied: rich in ./.venv/lib/python3.12/site-packages (from trl->-r requirements.txt (line 7)) (14.1.0)
Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft->-r requirements.txt (line 8)) (7.0.0)
Requirement already satisfied: click!=8.0.0,>=7.1 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (8.2.1)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (3.1.45)
Requirement already satisfied: platformdirs in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (4.3.8)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (6.31.1)
Requirement already satisfied: pydantic<3 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (2.11.7)
Requirement already satisfied: sentry-sdk>=2.0.0 in ./.venv/lib/python3.12/site-packages (from wandb->-r requirements.txt (line 9)) (2.34.1)
Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (0.7.0)
Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (2.33.2)
Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (0.4.1)
Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.2)
Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.5.0)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2025.8.3)
Requirement already satisfied: rustworkx>=0.15.0 in ./.venv/lib/python3.12/site-packages (from qiskit->-r requirements.txt (line 12)) (0.16.0)
Requirement already satisfied: scipy>=1.5 in ./.venv/lib/python3.12/site-packages (from qiskit->-r requirements.txt (line 12)) (1.16.1)
Requirement already satisfied: stevedore>=3.0.0 in ./.venv/lib/python3.12/site-packages (from qiskit->-r requirements.txt (line 12)) (5.4.1)
Requirement already satisfied: openqasm3<2.0,>=0.4 in ./.venv/lib/python3.12/site-packages (from openqasm3[parser]<2.0,>=0.4->qiskit_qasm3_import->-r requirements.txt (line 15)) (1.0.1)
Requirement already satisfied: antlr4_python3_runtime<4.14,>=4.7 in ./.venv/lib/python3.12/site-packages (from openqasm3[parser]<2.0,>=0.4->qiskit_qasm3_import->-r requirements.txt (line 15)) (4.13.2)
Requirement already satisfied: cachetools in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (6.1.0)
Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.2.0)
Requirement already satisfied: blake3 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.0.5)
Requirement already satisfied: py-cpuinfo in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (9.0.0)
Requirement already satisfied: fastapi>=0.115.0 in ./.venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.116.1)
Requirement already satisfied: openai<=1.90.0,>=1.87.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.90.0)
Requirement already satisfied: prometheus_client>=0.18.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.22.1)
Requirement already satisfied: pillow in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (11.3.0)
Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (7.1.0)
Requirement already satisfied: tiktoken>=0.6.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.10.0)
Requirement already satisfied: lm-format-enforcer<0.11,>=0.10.11 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.10.12)
Requirement already satisfied: llguidance<0.8.0,>=0.7.11 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.7.30)
Requirement already satisfied: outlines_core==0.2.10 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.2.10)
Requirement already satisfied: diskcache==5.6.3 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (5.6.3)
Requirement already satisfied: lark==1.2.2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.2.2)
Requirement already satisfied: xgrammar==0.1.21 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.1.21)
Requirement already satisfied: partial-json-parser in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.2.1.1.post6)
Requirement already satisfied: pyzmq>=25.0.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (27.0.1)
Requirement already satisfied: msgspec in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.19.0)
Requirement already satisfied: gguf>=0.13.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.17.1)
Requirement already satisfied: mistral_common>=1.8.2 in ./.venv/lib/python3.12/site-packages (from mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (1.8.3)
Requirement already satisfied: opencv-python-headless>=4.11.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (4.12.0.88)
Requirement already satisfied: six>=1.16.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.17.0)
Requirement already satisfied: einops in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.8.1)
Requirement already satisfied: compressed-tensors==0.10.2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.10.2)
Requirement already satisfied: depyf==0.19.0 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.19.0)
Requirement already satisfied: cloudpickle in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (3.1.1)
Requirement already satisfied: watchfiles in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.1.0)
Requirement already satisfied: python-json-logger in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (3.3.0)
Requirement already satisfied: ninja in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.11.1.4)
Requirement already satisfied: pybase64 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (1.4.2)
Requirement already satisfied: cbor2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (5.6.5)
Requirement already satisfied: numba==0.61.2 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.61.2)
Requirement already satisfied: torchaudio==2.7.1 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (2.7.1)
Requirement already satisfied: torchvision==0.22.1 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.22.1)
Requirement already satisfied: xformers==0.0.31 in ./.venv/lib/python3.12/site-packages (from vllm->-r requirements.txt (line 16)) (0.0.31)
Requirement already satisfied: astor in ./.venv/lib/python3.12/site-packages (from depyf==0.19.0->vllm->-r requirements.txt (line 16)) (0.8.1)
Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba==0.61.2->vllm->-r requirements.txt (line 16)) (0.44.0)
Requirement already satisfied: interegular>=0.3.2 in ./.venv/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm->-r requirements.txt (line 16)) (0.3.3)
Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (4.10.0)
Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (1.9.0)
Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (0.28.1)
Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (0.10.0)
Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (1.3.1)
Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (1.0.9)
Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<=1.90.0,>=1.87.0->vllm->-r requirements.txt (line 16)) (0.16.0)
Requirement already satisfied: hjson in ./.venv/lib/python3.12/site-packages (from deepspeed->-r requirements.txt (line 17)) (3.1.0)
Requirement already satisfied: msgpack in ./.venv/lib/python3.12/site-packages (from deepspeed->-r requirements.txt (line 17)) (1.1.1)
Requirement already satisfied: jsonschema in ./.venv/lib/python3.12/site-packages (from ray->-r requirements.txt (line 18)) (4.25.0)
Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2.6.1)
Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (25.3.0)
Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.7.0)
Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (6.6.3)
Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (0.3.2)
Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (1.20.1)
Requirement already satisfied: starlette<0.48.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.47.2)
Requirement already satisfied: fastapi-cli>=0.0.8 in ./.venv/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.0.8)
Requirement already satisfied: python-multipart>=0.0.18 in ./.venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.0.20)
Requirement already satisfied: email-validator>=2.0.0 in ./.venv/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (2.2.0)
Requirement already satisfied: uvicorn>=0.12.0 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.35.0)
Requirement already satisfied: dnspython>=2.0.0 in ./.venv/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (2.7.0)
Requirement already satisfied: typer>=0.15.1 in ./.venv/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.16.0)
Requirement already satisfied: rich-toolkit>=0.14.8 in ./.venv/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.14.9)
Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in ./.venv/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.1.5)
Requirement already satisfied: rignore>=0.5.1 in ./.venv/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.6.4)
Requirement already satisfied: gitdb<5,>=4.0.1 in ./.venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (4.0.12)
Requirement already satisfied: smmap<6,>=3.0.1 in ./.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (5.0.2)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->-r requirements.txt (line 5)) (3.0.2)
Requirement already satisfied: pydantic-extra-types>=2.10.5 in ./.venv/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (2.10.5)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.12/site-packages (from jsonschema->ray->-r requirements.txt (line 18)) (2025.4.1)
Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.12/site-packages (from jsonschema->ray->-r requirements.txt (line 18)) (0.36.2)
Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.12/site-packages (from jsonschema->ray->-r requirements.txt (line 18)) (0.27.0)
Requirement already satisfied: pycountry>=23 in ./.venv/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (24.6.1)
Requirement already satisfied: cupy-cuda12x in ./.venv/lib/python3.12/site-packages (from ray[cgraph]!=2.44.*,>=2.43.0->vllm->-r requirements.txt (line 16)) (13.5.1)
Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich->trl->-r requirements.txt (line 7)) (3.0.0)
Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich->trl->-r requirements.txt (line 7)) (2.19.2)
Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 7)) (0.1.2)
Requirement already satisfied: pbr>=2.0.0 in ./.venv/lib/python3.12/site-packages (from stevedore>=3.0.0->qiskit->-r requirements.txt (line 12)) (6.1.1)
Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (1.5.4)
Requirement already satisfied: httptools>=0.6.3 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.6.4)
Requirement already satisfied: python-dotenv>=0.13 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (1.1.1)
Requirement already satisfied: uvloop>=0.15.1 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (0.21.0)
Requirement already satisfied: websockets>=10.4 in ./.venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == "standard"->fastapi[standard]>=0.115.0->vllm->-r requirements.txt (line 16)) (15.0.1)
Requirement already satisfied: fastrlock>=0.5 in ./.venv/lib/python3.12/site-packages (from cupy-cuda12x->ray[cgraph]!=2.44.*,>=2.43.0->vllm->-r requirements.txt (line 16)) (0.8.3)
Requirement already satisfied: soundfile>=0.12.1 in ./.venv/lib/python3.12/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (0.13.1)
Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (1.17.1)
Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (2.22)
Requirement already satisfied: soxr>=0.5.0 in ./.venv/lib/python3.12/site-packages (from mistral_common>=1.8.2->mistral_common[audio,image]>=1.8.2->vllm->-r requirements.txt (line 16)) (0.5.0.post1)
ipex flag is deprecated, will be removed in Accelerate v1.10. From 2.7.0, PyTorch has all needed optimizations for Intel CPU and XPU.
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,265 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-16_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 0, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,537 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-15_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 7, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,544 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-16_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 5, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,550 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-15_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 2, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,568 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-15_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 1, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
2025-08-11 12:24:17,569 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-16_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 6, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,584 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-16_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 3, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
2025-08-11 12:24:17,586 - INFO - Parsed config: {'model_name': 'Qwen/Qwen3-8B', 'block_size': 12288, 'wandb_project': 'quantum-circuit-generation', 'train_file_path': 'linuzj/graph-data-quantum-tokenized_sft', 'output_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'eval_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 1, 'per_device_eval_batch_size': 1, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'torch_empty_cache_steps': None, 'learning_rate': 2e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 15.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'lr_scheduler_kwargs': {}, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'info', 'log_level_replica': 'warning', 'log_on_each_node': True, 'logging_dir': '/scratch/cs/adis/yuc10/quantum-code-generation/code/training/experiments/sft_quantum_circuit_gen_8B_20250811_122403/runs/Aug11_12-24-16_gpu59.int.triton.aalto.fi', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <SaveStrategy.STEPS: 'steps'>, 'save_steps': 24000, 'save_total_limit': 1, 'save_safetensors': True, 'save_on_each_node': False, 'save_only_model': False, 'restore_callback_states_from_checkpoint': False, 'no_cuda': False, 'use_cpu': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': True, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': 4, 'ddp_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'dataloader_prefetch_factor': None, 'past_index': -1, 'run_name': None, 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'fsdp': [<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>], 'fsdp_min_num_params': 0, 'fsdp_config': {'xla': False, 'enable_forward_prefetch': True, 'limit_all_gathers': True, 'activation_checkpointing': False, 'activation_offload': False, 'state_dict_type': 'FULL_STATE_DICT', 'use_orig_params': True, 'transformer_layer_cls_to_wrap': ['Qwen3DecoderLayer'], 'auto_wrap_policy': 'TRANSFORMER_BASED_WRAP', 'min_num_params': 0, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, 'fsdp_transformer_layer_cls_to_wrap': None, 'accelerator_config': {'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False}, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['wandb'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'ddp_broadcast_buffers': None, 'dataloader_pin_memory': True, 'dataloader_persistent_workers': False, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': True, 'resume_from_checkpoint': None, 'hub_model_id': 'Benyucong/sft_quantum_circuit_gen_8B', 'hub_strategy': <HubStrategy.ALL_CHECKPOINTS: 'all_checkpoints'>, 'hub_token': None, 'hub_private_repo': None, 'hub_always_push': False, 'hub_revision': None, 'gradient_checkpointing': False, 'gradient_checkpointing_kwargs': None, 'include_inputs_for_metrics': False, 'include_for_metrics': [], 'eval_do_concat_batches': True, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, '_n_gpu': 1, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, 'include_tokens_per_second': False, 'include_num_input_tokens_seen': False, 'neftune_noise_alpha': None, 'optim_target_modules': None, 'batch_eval_metrics': False, 'eval_on_start': False, 'use_liger_kernel': False, 'liger_kernel_config': None, 'eval_use_gather_object': False, 'average_tokens_across_devices': True, 'model_init_kwargs': None, 'use_liger': False, 'dataset_text_field': 'text', 'dataset_kwargs': None, 'dataset_num_proc': None, 'max_seq_length': 12288, 'packing': False, 'eval_packing': None, 'dataset_batch_size': None, 'num_of_sequences': None, 'chars_per_token': None}
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading file vocab.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/vocab.json
loading file merges.txt from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/merges.txt
loading file tokenizer.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 149.99it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 156.65it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 156.94it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 162.41it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 161.42it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 158.68it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/config.json
Model config Qwen3Config {
  "architectures": [
    "Qwen3ForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 12288,
  "layer_types": [
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention",
    "full_attention"
  ],
  "max_position_embeddings": 40960,
  "max_window_layers": 36,
  "model_type": "qwen3",
  "num_attention_heads": 32,
  "num_hidden_layers": 36,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.55.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/model.safetensors.index.json
Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 158.78it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
Loading checkpoint shards: 100%|██████████| 5/5 [00:00<00:00, 158.75it/s]
All model checkpoint weights were used when initializing Qwen3ForCausalLM.

All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.
loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

loading configuration file generation_config.json from cache at /scratch/cs/adis/yuc10/quantum-code-generation/code/training/hf/hub/models--Qwen--Qwen3-8B/snapshots/b968826d9c46dd6066d109eabc6255188de91218/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "temperature": 0.6,
  "top_k": 20,
  "top_p": 0.95
}

[2025-08-11 12:24:23,209] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-08-11 12:24:23,351] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:24:23,352] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:24:23,352] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:24:23,352] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:24:23,352] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:24:23,353] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-08-11 12:24:23,388] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.

Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.


Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Warning: The cache directory for DeepSpeed Triton autotune, /home/yuc10/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-08-11 12:24:24,225] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,344] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,344] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,345] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,345] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,347] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,362] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
[2025-08-11 12:24:24,370] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
Using auto half precision backend
2025-08-11 12:24:24,372 - INFO - Resume checkpoint: None (fresh run)
The following columns in the Training set don't have a corresponding argument in `Qwen3ForCausalLM.forward` and have been ignored: graph, number_of_layers, ansatz_id, text, circuit_with_symbols, signature, solution, optimization_type, cost_hamiltonian, problem_specific_attributes, circuit_with_params, exact_solution, number_of_qubits, adaptive_process, problem_type. If graph, number_of_layers, ansatz_id, text, circuit_with_symbols, signature, solution, optimization_type, cost_hamiltonian, problem_specific_attributes, circuit_with_params, exact_solution, number_of_qubits, adaptive_process, problem_type are not expected by `Qwen3ForCausalLM.forward`,  you can safely ignore this message.
/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1805: UserWarning: Upcasted low precision parameters in Qwen3ForCausalLM because mixed precision turned on in FSDP. Affects: model.embed_tokens.weight, model.norm.weight, lm_head.weight.
  warnings.warn(
/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1805: UserWarning: Upcasted low precision parameters in Qwen3DecoderLayer because mixed precision turned on in FSDP. Affects: self_attn.q_proj.weight, self_attn.k_proj.weight, self_attn.v_proj.weight, self_attn.o_proj.weight, self_attn.q_norm.weight, self_attn.k_norm.weight, mlp.gate_proj.weight, mlp.up_proj.weight, mlp.down_proj.weight, input_layernorm.weight, post_attention_layernorm.weight.
  warnings.warn(
/scratch/cs/adis/yuc10/quantum-code-generation/code/training/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:1811: UserWarning: FSDP upcast of low precision parameters may affect the precision of model checkpoints.
  warnings.warn(
***** Running training *****
  Num examples = 13,914
  Num Epochs = 15
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 26,100
  Number of trainable parameters = 1,023,841,920
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: benyucong (benyucong-aalto-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.21.0
wandb: Run data is saved locally in /scratch/cs/adis/yuc10/quantum-code-generation/code/training/wandb/run-20250811_122447-v7qf8g4o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-terrain-4
wandb: ⭐️ View project at https://wandb.ai/benyucong-aalto-university/quantum-circuit-generation
wandb: 🚀 View run at https://wandb.ai/benyucong-aalto-university/quantum-circuit-generation/runs/v7qf8g4o
  0%|          | 0/26100 [00:00<?, ?it/s]  0%|          | 1/26100 [00:05<38:26:11,  5.30s/it]  0%|          | 2/26100 [00:06<21:47:54,  3.01s/it]  0%|          | 3/26100 [00:08<16:39:24,  2.30s/it]  0%|          | 4/26100 [00:08<11:49:56,  1.63s/it]  0%|          | 5/26100 [00:09<10:27:14,  1.44s/it]  0%|          | 6/26100 [00:10<8:25:36,  1.16s/it]   0%|          | 7/26100 [00:11<7:35:15,  1.05s/it]  0%|          | 8/26100 [00:12<6:47:01,  1.07it/s]  0%|          | 9/26100 [00:12<6:52:52,  1.05it/s]  0%|          | 10/26100 [00:14<7:32:43,  1.04s/it]  0%|          | 11/26100 [00:15<8:49:25,  1.22s/it]  0%|          | 12/26100 [00:16<7:38:07,  1.05s/it]  0%|          | 13/26100 [00:17<7:05:26,  1.02it/s]  0%|          | 14/26100 [00:17<5:59:03,  1.21it/s]  0%|          | 15/26100 [00:18<5:47:33,  1.25it/s]  0%|          | 16/26100 [00:19<5:20:40,  1.36it/s]  0%|          | 17/26100 [00:19<4:51:49,  1.49it/s]  0%|          | 18/26100 [00:20<5:02:40,  1.44it/s]  0%|          | 19/26100 [00:20<4:37:19,  1.57it/s]  0%|          | 20/26100 [00:21<5:28:42,  1.32it/s]  0%|          | 21/26100 [00:22<5:05:49,  1.42it/s]  0%|          | 22/26100 [00:23<5:13:11,  1.39it/s]  0%|          | 23/26100 [00:24<6:12:06,  1.17it/s]  0%|          | 24/26100 [00:25<6:48:07,  1.06it/s]  0%|          | 25/26100 [00:26<6:10:25,  1.17it/s]  0%|          | 26/26100 [00:26<5:47:58,  1.25it/s]  0%|          | 27/26100 [00:28<7:24:48,  1.02s/it]  0%|          | 28/26100 [00:29<7:32:43,  1.04s/it]  0%|          | 29/26100 [00:29<6:16:06,  1.16it/s]  0%|          | 30/26100 [00:30<5:53:53,  1.23it/s]  0%|          | 31/26100 [00:31<5:27:05,  1.33it/s]  0%|          | 32/26100 [00:32<7:28:23,  1.03s/it]  0%|          | 33/26100 [00:33<6:25:30,  1.13it/s]  0%|          | 34/26100 [00:33<5:24:58,  1.34it/s]  0%|          | 35/26100 [00:34<4:49:04,  1.50it/s]  0%|          | 36/26100 [00:34<4:33:52,  1.59it/s]  0%|          | 37/26100 [00:35<4:50:58,  1.49it/s]  0%|          | 38/26100 [00:36<5:42:24,  1.27it/s]  0%|          | 39/26100 [00:37<5:21:40,  1.35it/s]  0%|          | 40/26100 [00:38<5:11:39,  1.39it/s]  0%|          | 41/26100 [00:38<4:47:06,  1.51it/s]  0%|          | 42/26100 [00:39<4:27:57,  1.62it/s]  0%|          | 43/26100 [00:39<4:33:09,  1.59it/s]  0%|          | 44/26100 [00:40<4:43:59,  1.53it/s]  0%|          | 45/26100 [00:42<6:36:55,  1.09it/s]  0%|          | 46/26100 [00:43<7:32:25,  1.04s/it]  0%|          | 47/26100 [00:45<9:01:13,  1.25s/it]  0%|          | 48/26100 [00:45<7:27:42,  1.03s/it]  0%|          | 49/26100 [00:46<6:39:27,  1.09it/s]  0%|          | 50/26100 [00:47<6:20:50,  1.14it/s]  0%|          | 51/26100 [00:47<6:02:14,  1.20it/s]  0%|          | 52/26100 [00:48<5:20:05,  1.36it/s]  0%|          | 53/26100 [00:49<6:02:41,  1.20it/s]  0%|          | 54/26100 [00:50<5:38:18,  1.28it/s]  0%|          | 55/26100 [00:50<5:08:29,  1.41it/s]  0%|          | 56/26100 [00:51<5:15:17,  1.38it/s]  0%|          | 57/26100 [00:52<6:45:12,  1.07it/s]  0%|          | 58/26100 [00:53<5:52:58,  1.23it/s]  0%|          | 59/26100 [00:54<5:44:41,  1.26it/s]  0%|          | 60/26100 [00:54<5:41:57,  1.27it/s]  0%|          | 61/26100 [00:57<9:01:31,  1.25s/it]  0%|          | 62/26100 [00:57<7:55:53,  1.10s/it]  0%|          | 63/26100 [00:58<6:46:04,  1.07it/s]  0%|          | 64/26100 [00:59<6:03:32,  1.19it/s]  0%|          | 65/26100 [00:59<5:46:58,  1.25it/s]  0%|          | 66/26100 [01:01<7:00:45,  1.03it/s]  0%|          | 67/26100 [01:01<6:15:09,  1.16it/s]  0%|          | 68/26100 [01:02<6:18:46,  1.15it/s]  0%|          | 69/26100 [01:03<6:06:43,  1.18it/s]  0%|          | 70/26100 [01:04<5:34:10,  1.30it/s]  0%|          | 71/26100 [01:05<6:14:49,  1.16it/s]  0%|          | 72/26100 [01:05<5:19:10,  1.36it/s]  0%|          | 73/26100 [01:06<5:09:09,  1.40it/s]  0%|          | 74/26100 [01:06<5:00:09,  1.45it/s]  0%|          | 75/26100 [01:07<4:50:01,  1.50it/s]  0%|          | 76/26100 [01:08<5:04:07,  1.43it/s]  0%|          | 77/26100 [01:08<4:53:43,  1.48it/s]  0%|          | 78/26100 [01:09<4:54:43,  1.47it/s]  0%|          | 79/26100 [01:10<5:07:04,  1.41it/s]  0%|          | 80/26100 [01:11<5:09:08,  1.40it/s]  0%|          | 81/26100 [01:11<4:38:06,  1.56it/s]  0%|          | 82/26100 [01:12<4:30:41,  1.60it/s]  0%|          | 83/26100 [01:12<4:10:18,  1.73it/s]  0%|          | 84/26100 [01:13<4:45:15,  1.52it/s]  0%|          | 85/26100 [01:14<4:43:31,  1.53it/s]  0%|          | 86/26100 [01:14<4:26:16,  1.63it/s]  0%|          | 87/26100 [01:15<4:39:10,  1.55it/s]  0%|          | 88/26100 [01:16<4:52:59,  1.48it/s]  0%|          | 89/26100 [01:16<4:40:37,  1.54it/s]  0%|          | 90/26100 [01:17<6:10:47,  1.17it/s]  0%|          | 91/26100 [01:18<5:36:46,  1.29it/s]  0%|          | 92/26100 [01:19<5:06:33,  1.41it/s]  0%|          | 93/26100 [01:19<5:32:10,  1.30it/s]  0%|          | 94/26100 [01:20<5:17:54,  1.36it/s]  0%|          | 95/26100 [01:21<5:16:26,  1.37it/s]  0%|          | 96/26100 [01:22<5:21:46,  1.35it/s]  0%|          | 97/26100 [01:22<5:10:33,  1.40it/s]  0%|          | 98/26100 [01:23<4:41:44,  1.54it/s]  0%|          | 99/26100 [01:24<5:13:52,  1.38it/s]  0%|          | 100/26100 [01:25<5:34:13,  1.30it/s]  0%|          | 101/26100 [01:25<5:31:07,  1.31it/s]  0%|          | 102/26100 [01:26<5:10:21,  1.40it/s]  0%|          | 103/26100 [01:27<5:11:34,  1.39it/s]  0%|          | 104/26100 [01:27<4:38:02,  1.56it/s]  0%|          | 105/26100 [01:28<4:45:55,  1.52it/s]  0%|          | 106/26100 [01:29<5:23:16,  1.34it/s]  0%|          | 107/26100 [01:29<4:49:50,  1.49it/s]  0%|          | 108/26100 [01:30<5:10:50,  1.39it/s]  0%|          | 109/26100 [01:31<5:01:31,  1.44it/s]  0%|          | 110/26100 [01:32<5:21:00,  1.35it/s]  0%|          | 111/26100 [01:32<5:33:10,  1.30it/s]  0%|          | 112/26100 [01:33<5:27:29,  1.32it/s]  0%|          | 113/26100 [01:34<4:55:45,  1.46it/s]  0%|          | 114/26100 [01:34<4:24:13,  1.64it/s]  0%|          | 115/26100 [01:35<4:38:43,  1.55it/s]  0%|          | 116/26100 [01:35<4:38:15,  1.56it/s]  0%|          | 117/26100 [01:36<4:49:30,  1.50it/s]  0%|          | 118/26100 [01:37<4:54:37,  1.47it/s]  0%|          | 119/26100 [01:37<4:36:40,  1.57it/s]  0%|          | 120/26100 [01:38<4:50:27,  1.49it/s]  0%|          | 121/26100 [01:39<4:47:04,  1.51it/s]  0%|          | 122/26100 [01:39<4:42:41,  1.53it/s]  0%|          | 123/26100 [01:40<4:44:46,  1.52it/s]  0%|          | 124/26100 [01:42<6:45:14,  1.07it/s]  0%|          | 125/26100 [01:43<6:41:39,  1.08it/s]  0%|          | 126/26100 [01:43<6:13:47,  1.16it/s]  0%|          | 127/26100 [01:44<5:29:23,  1.31it/s]  0%|          | 128/26100 [01:45<6:14:22,  1.16it/s]  0%|          | 129/26100 [01:46<5:35:20,  1.29it/s]  0%|          | 130/26100 [01:46<5:57:50,  1.21it/s]  1%|          | 131/26100 [01:48<7:09:15,  1.01it/s]  1%|          | 132/26100 [01:49<7:20:43,  1.02s/it]  1%|          | 133/26100 [01:50<7:08:59,  1.01it/s]  1%|          | 134/26100 [01:50<6:01:15,  1.20it/s]  1%|          | 135/26100 [01:51<5:57:14,  1.21it/s]  1%|          | 136/26100 [01:52<5:46:22,  1.25it/s]  1%|          | 137/26100 [01:52<5:16:45,  1.37it/s]  1%|          | 138/26100 [01:53<4:46:53,  1.51it/s]  1%|          | 139/26100 [01:54<4:53:26,  1.47it/s]  1%|          | 140/26100 [01:55<6:51:23,  1.05it/s]  1%|          | 141/26100 [01:56<6:14:30,  1.16it/s]  1%|          | 142/26100 [01:57<5:49:13,  1.24it/s]  1%|          | 143/26100 [01:57<5:20:42,  1.35it/s]  1%|          | 144/26100 [01:58<4:48:50,  1.50it/s]  1%|          | 145/26100 [01:58<4:45:55,  1.51it/s]  1%|          | 146/26100 [01:59<4:47:43,  1.50it/s]  1%|          | 147/26100 [02:00<6:16:39,  1.15it/s]  1%|          | 148/26100 [02:01<5:23:33,  1.34it/s]  1%|          | 149/26100 [02:01<4:59:21,  1.44it/s]  1%|          | 150/26100 [02:02<4:45:32,  1.51it/s]  1%|          | 151/26100 [02:02<4:32:15,  1.59it/s]  1%|          | 152/26100 [02:03<4:30:18,  1.60it/s]  1%|          | 153/26100 [02:04<4:24:42,  1.63it/s]  1%|          | 154/26100 [02:06<7:09:21,  1.01it/s]  1%|          | 155/26100 [02:06<6:25:10,  1.12it/s]  1%|          | 156/26100 [02:07<6:42:13,  1.07it/s]  1%|          | 157/26100 [02:08<6:16:42,  1.15it/s]  1%|          | 158/26100 [02:09<7:13:06,  1.00s/it]  1%|          | 159/26100 [02:10<6:14:09,  1.16it/s]  1%|          | 160/26100 [02:11<6:00:04,  1.20it/s]  1%|          | 161/26100 [02:12<6:38:24,  1.09it/s]  1%|          | 162/26100 [02:12<6:08:56,  1.17it/s]  1%|          | 163/26100 [02:13<5:34:32,  1.29it/s]  1%|          | 164/26100 [02:14<5:05:27,  1.42it/s]  1%|          | 165/26100 [02:14<4:36:52,  1.56it/s]  1%|          | 166/26100 [02:15<4:34:45,  1.57it/s]  1%|          | 167/26100 [02:16<6:01:11,  1.20it/s]  1%|          | 168/26100 [02:17<6:53:59,  1.04it/s]  1%|          | 169/26100 [02:19<8:40:13,  1.20s/it]  1%|          | 170/26100 [02:20<7:37:53,  1.06s/it]  1%|          | 171/26100 [02:20<6:26:09,  1.12it/s]  1%|          | 172/26100 [02:21<6:50:03,  1.05it/s]  1%|          | 173/26100 [02:22<6:07:45,  1.18it/s]  1%|          | 174/26100 [02:22<5:28:50,  1.31it/s]  1%|          | 175/26100 [02:23<5:25:02,  1.33it/s]  1%|          | 176/26100 [02:24<4:58:59,  1.45it/s]  1%|          | 177/26100 [02:24<4:40:17,  1.54it/s]  1%|          | 178/26100 [02:25<5:23:22,  1.34it/s]  1%|          | 179/26100 [02:26<5:16:01,  1.37it/s]  1%|          | 180/26100 [02:27<5:16:01,  1.37it/s]  1%|          | 181/26100 [02:28<6:46:28,  1.06it/s]  1%|          | 182/26100 [02:30<7:42:11,  1.07s/it]  1%|          | 183/26100 [02:32<9:53:30,  1.37s/it]  1%|          | 184/26100 [02:32<8:20:42,  1.16s/it]  1%|          | 185/26100 [02:33<7:12:06,  1.00s/it]  1%|          | 186/26100 [02:35<8:33:29,  1.19s/it]  1%|          | 187/26100 [02:36<8:44:30,  1.21s/it]  1%|          | 188/26100 [02:36<7:24:06,  1.03s/it]  1%|          | 189/26100 [02:38<8:12:46,  1.14s/it]  1%|          | 190/26100 [02:38<7:01:15,  1.03it/s]  1%|          | 191/26100 [02:39<6:13:35,  1.16it/s]  1%|          | 192/26100 [02:39<5:13:55,  1.38it/s]  1%|          | 193/26100 [02:40<5:13:54,  1.38it/s]  1%|          | 194/26100 [02:41<5:45:17,  1.25it/s]  1%|          | 195/26100 [02:42<5:27:08,  1.32it/s]  1%|          | 196/26100 [02:42<5:18:38,  1.35it/s]  1%|          | 197/26100 [02:43<4:51:54,  1.48it/s]  1%|          | 198/26100 [02:44<6:42:03,  1.07it/s]  1%|          | 199/26100 [02:45<6:17:54,  1.14it/s]  1%|          | 200/26100 [02:46<5:45:10,  1.25it/s]  1%|          | 201/26100 [02:47<6:20:25,  1.13it/s]  1%|          | 202/26100 [02:47<5:40:12,  1.27it/s]  1%|          | 203/26100 [02:49<6:45:06,  1.07it/s]  1%|          | 204/26100 [02:50<6:20:37,  1.13it/s]  1%|          | 205/26100 [02:50<5:32:39,  1.30it/s]  1%|          | 206/26100 [02:51<5:15:35,  1.37it/s]  1%|          | 207/26100 [02:51<4:54:26,  1.47it/s]  1%|          | 208/26100 [02:52<4:54:15,  1.47it/s]  1%|          | 209/26100 [02:53<4:43:18,  1.52it/s]  1%|          | 210/26100 [02:53<4:39:18,  1.54it/s]  1%|          | 211/26100 [02:54<5:20:11,  1.35it/s]  1%|          | 212/26100 [02:55<6:30:17,  1.11it/s]  1%|          | 213/26100 [02:56<5:53:47,  1.22it/s]  1%|          | 214/26100 [02:57<5:56:48,  1.21it/s]  1%|          | 215/26100 [02:57<5:28:23,  1.31it/s]  1%|          | 216/26100 [02:58<5:31:24,  1.30it/s]  1%|          | 217/26100 [02:59<5:17:22,  1.36it/s]  1%|          | 218/26100 [02:59<4:44:40,  1.52it/s]  1%|          | 219/26100 [03:00<4:26:46,  1.62it/s]  1%|          | 220/26100 [03:01<4:41:42,  1.53it/s]  1%|          | 221/26100 [03:01<4:55:27,  1.46it/s]  1%|          | 222/26100 [03:02<5:09:25,  1.39it/s]  1%|          | 223/26100 [03:03<4:36:48,  1.56it/s]  1%|          | 224/26100 [03:03<4:25:52,  1.62it/s]  1%|          | 225/26100 [03:04<4:25:46,  1.62it/s]  1%|          | 226/26100 [03:05<5:53:07,  1.22it/s]  1%|          | 227/26100 [03:06<5:12:02,  1.38it/s]  1%|          | 228/26100 [03:07<6:54:55,  1.04it/s]  1%|          | 229/26100 [03:08<6:06:16,  1.18it/s]  1%|          | 230/26100 [03:08<5:28:34,  1.31it/s]  1%|          | 231/26100 [03:09<5:00:54,  1.43it/s]  1%|          | 232/26100 [03:09<4:53:03,  1.47it/s]  1%|          | 233/26100 [03:10<4:44:25,  1.52it/s]  1%|          | 234/26100 [03:11<5:00:02,  1.44it/s]  1%|          | 235/26100 [03:12<4:51:22,  1.48it/s]  1%|          | 236/26100 [03:12<4:46:50,  1.50it/s]  1%|          | 237/26100 [03:14<6:25:49,  1.12it/s]  1%|          | 238/26100 [03:14<5:46:27,  1.24it/s]  1%|          | 239/26100 [03:15<5:14:07,  1.37it/s]  1%|          | 240/26100 [03:16<5:36:17,  1.28it/s]  1%|          | 241/26100 [03:16<4:55:14,  1.46it/s]  1%|          | 242/26100 [03:17<5:23:44,  1.33it/s]  1%|          | 243/26100 [03:18<4:59:07,  1.44it/s]  1%|          | 244/26100 [03:18<5:12:27,  1.38it/s]  1%|          | 245/26100 [03:19<4:45:25,  1.51it/s]  1%|          | 246/26100 [03:21<6:58:30,  1.03it/s]  1%|          | 247/26100 [03:22<8:47:29,  1.22s/it]  1%|          | 248/26100 [03:24<8:58:03,  1.25s/it]  1%|          | 249/26100 [03:25<8:05:29,  1.13s/it]  1%|          | 250/26100 [03:25<6:54:05,  1.04it/s]  1%|          | 251/26100 [03:26<6:09:26,  1.17it/s]  1%|          | 252/26100 [03:26<5:33:29,  1.29it/s]  1%|          | 253/26100 [03:27<5:48:48,  1.24it/s]  1%|          | 254/26100 [03:29<7:24:36,  1.03s/it]  1%|          | 255/26100 [03:29<6:15:39,  1.15it/s]  1%|          | 256/26100 [03:30<6:16:17,  1.14it/s]  1%|          | 257/26100 [03:31<5:33:35,  1.29it/s]  1%|          | 258/26100 [03:31<5:23:08,  1.33it/s]slurmstepd: error: *** JOB 9837421 ON gpu59 CANCELLED AT 2025-08-11T12:28:21 ***
  1%|          | 259/26100 [03:32<5:07:59,  1.40it/s]  1%|          | 260/26100 [03:33<6:38:39,  1.08it/s]  1%|          | 261/26100 [03:34<5:56:29,  1.21it/s]  1%|          | 262/26100 [03:35<5:18:31,  1.35it/s]  1%|          | 263/26100 [03:35<5:05:54,  1.41it/s]  1%|          | 264/26100 [03:36<5:02:24,  1.42it/s]  1%|          | 265/26100 [03:36<4:23:06,  1.64it/s]  1%|          | 266/26100 [03:37<5:18:47,  1.35it/s]